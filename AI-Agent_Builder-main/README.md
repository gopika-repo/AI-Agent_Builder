<div align="center">

<br>

# ğŸ§  AI Agent Builder â€” Multi-Modal Document Intelligence Platform

### _Construct Knowledge from Chaos_

<br>

[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18+-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://reactjs.org)
[![LangGraph](https://img.shields.io/badge/LangGraph-Agentic_AI-FF4B4B?style=for-the-badge&logo=langchain&logoColor=white)](https://langchain.com)
[![CrewAI](https://img.shields.io/badge/CrewAI-Multi_Agent-FF6B35?style=for-the-badge)](https://www.crewai.com)
[![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-D12D6F?style=for-the-badge)](https://qdrant.tech)
[![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?style=for-the-badge&logo=githubactions&logoColor=white)](https://github.com/features/actions)
[![YOLOv8](https://img.shields.io/badge/YOLOv8-Computer_Vision-00FFFF?style=for-the-badge)](https://docs.ultralytics.com)

---

<h3>ğŸ† A production-grade, 6-agent AI system that <b>sees</b>, <b>reads</b>, <b>reasons</b>, <b>debates</b>, and <b>self-heals</b><br>to extract near-perfect insights from any document.</h3>

<br>

| ğŸ“„ Upload Any Document | ğŸ¤– 6 AI Agents Collaborate | ğŸ’¡ Get Expert Answers |
|:---:|:---:|:---:|
| PDF, PNG, JPG, scans | Vision Â· OCR Â· Layout Â· Reasoning Â· Fusion Â· Validation | ELI5 or Expert mode with source citations |

</div>

<br>

---

## ğŸ“– Table of Contents

- [Why This Platform?](#-why-this-platform)
- [Key Capabilities](#-key-capabilities)
- [System Architecture](#-system-architecture)
- [Agent Pipeline Flowchart](#-agent-pipeline-flowchart)
- [Conflict Resolution Deep Dive](#-conflict-resolution-deep-dive)
- [RAG Query Flow](#-rag-query-flow)
- [Technology Stack](#-technology-stack)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [API Reference](#-api-reference)
- [Running Tests](#-running-tests)
- [Deployment](#-deployment)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸš€ Why This Platform?

Traditional document processing breaks when documents get complex â€” **tables span pages, charts contain critical data, headers get misread, and OCR garbles structured layouts.**

Most tools use a single linear pipeline: `OCR â†’ Text â†’ Done`. That's not good enough.

**We built a 6-agent collaborative AI system that works like a team of expert analysts:**

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                                 â”‚
  â”‚   ğŸ‘ï¸  SEE it        â†’  Computer Vision detects layout regions   â”‚
  â”‚   ğŸ“  READ it       â†’  Dual OCR engines extract every word      â”‚
  â”‚   ğŸ“  STRUCTURE it  â†’  Layout analysis reconstructs tables      â”‚
  â”‚   ğŸ§   REASON about  â†’  LLM understands context & meaning        â”‚
  â”‚   âš—ï¸  DEBATE it     â†’  Conflict Resolution fuses disagreements  â”‚
  â”‚   âœ…  VERIFY it     â†’  Validation + Human-in-the-loop QA        â”‚
  â”‚                                                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Result:** Where traditional OCR pipelines achieve ~70-80% accuracy on complex documents, our conflict-aware fusion system targets **99%+ accuracy** by resolving disagreements between modalities.

---

## ğŸ† Key Capabilities

<table>
<tr>
<td width="33%">

### ğŸ§  Intelligent Processing
- **Conflict-Aware Fusion** â€” Auto-detects when OCR disagrees with Vision and resolves via semantic logic
- **Table Intelligence** â€” Reconstructs complex tables, performs SQL-like queries (Sum, Trend, Filter)
- **Multi-Modal RAG** â€” Cross-references text, tables, and images simultaneously

</td>
<td width="33%">

### âš¡ Enterprise-Grade
- **Self-Healing Pipeline** â€” Auto-retries failed agents with fallback strategies
- **Hardware Agnostic** â€” Runs on CPU (quantized) or GPU (full precision)
- **Containerized** â€” Docker + Docker Compose, Kubernetes-ready

</td>
<td width="33%">

### ğŸ¨ Premium UX
- **Visual Grounding** â€” Highlights exact source regions on documents
- **Confidence Heatmaps** â€” Color-coded overlays for low-confidence areas
- **ELI5 vs Expert** â€” Toggle between simple and technical explanations
- **Voice Queries** â€” Ask questions by speaking

</td>
</tr>
</table>

---

## ğŸ—ï¸ System Architecture

The platform uses a **hybrid LangGraph + CrewAI** architecture. LangGraph provides deterministic state-machine control over the 6-agent pipeline, while CrewAI orchestrates higher-level analytical crews (Analyst, Fact-Checker, Summarizer).

```mermaid
graph TB
    subgraph CLIENT["ğŸ–¥ï¸ Frontend â€” React + Vite"]
        UI_UP["ğŸ“¤ Document Upload"]
        UI_CHAT["ğŸ’¬ Chat Interface"]
        UI_VIEW["ğŸ“„ Document Viewer"]
        UI_HEAT["ğŸŒ¡ï¸ Confidence Heatmap"]
        UI_REV["ğŸ“‹ Review Panel"]
        UI_COMP["ğŸ” Multi-Doc Compare"]
    end

    subgraph API["âš¡ API Layer â€” FastAPI"]
        REST["REST Endpoints"]
        WS["WebSocket Server"]
    end

    subgraph PIPELINE["ğŸ¤– LangGraph Agent Pipeline"]
        direction LR
        V["ğŸ‘ï¸ Vision"]
        O["ğŸ“ OCR"]
        L["ğŸ“ Layout"]
        R["ğŸ§  Reasoning"]
        F["âš—ï¸ Fusion"]
        VAL["âœ… Validation"]
    end

    subgraph INTELLIGENCE["ğŸ§ª Intelligence Layer"]
        CR["âš¡ Conflict Resolution"]
        SH["ğŸ¥ Self-Healing"]
        CREW["ğŸ‘¥ CrewAI Crews"]
        TR["ğŸ“Š Table Reasoning"]
    end

    subgraph STORAGE["ğŸ’¾ Storage Layer"]
        QD[("ğŸ”® Qdrant\nVector DB")]
        FS[("ğŸ“ File System\nUploads")]
    end

    subgraph RAG["ğŸ” RAG Pipeline"]
        EMB["Embeddings Engine"]
        RET["Cross-Modal Retriever"]
    end

    CLIENT <-->|HTTP / WebSocket| API
    API --> PIPELINE
    PIPELINE --> INTELLIGENCE
    PIPELINE --> STORAGE
    RAG --> QD
    UI_CHAT --> RAG
    F -.-> CR
    PIPELINE -.-> SH

    style CLIENT fill:#1a1a2e,stroke:#16213e,color:#e94560
    style API fill:#0f3460,stroke:#16213e,color:#e94560
    style PIPELINE fill:#533483,stroke:#16213e,color:#e94560
    style INTELLIGENCE fill:#2b2d42,stroke:#16213e,color:#e94560
    style STORAGE fill:#1b1b2f,stroke:#16213e,color:#e94560
    style RAG fill:#2d3436,stroke:#16213e,color:#e94560
```

---

## ğŸ”„ Agent Pipeline Flowchart

This is the core document processing pipeline. Each agent has defined transitions, error recovery paths, and conditional routing based on processing results.

```mermaid
flowchart TD
    START(("ğŸ“¥ Document\nUploaded")) --> V

    V["ğŸ‘ï¸ <b>Vision Agent</b>\n<i>YOLOv8 Detection</i>"]
    V -->|Success| O
    V -->|Failure| ERR

    O["ğŸ“ <b>OCR Agent</b>\n<i>Tesseract + EasyOCR</i>"]
    O -->|Success| L
    O -->|Failure| ERR

    L["ğŸ“ <b>Layout Agent</b>\n<i>Structure Analysis</i>"]
    L -->|Success| R
    L -->|Failure| ERR

    R["ğŸ§  <b>Reasoning Agent</b>\n<i>LLM-Powered Analysis</i>"]
    R -->|Success| F
    R -->|Failure| ERR

    F["âš—ï¸ <b>Fusion Agent</b>\n<i>Cross-Modal Merge</i>"]
    F -->|Conflict Detected| CR
    F -->|Clean Merge| VAL

    CR["âš¡ <b>Conflict Resolution</b>\n<i>Semantic Arbitration</i>"]
    CR --> VAL

    VAL["âœ… <b>Validation Agent</b>\n<i>Quality Assurance</i>"]
    VAL -->|"âœ… Confidence â‰¥ 0.85"| STORE
    VAL -->|"âš ï¸ Confidence < 0.85"| HUMAN
    VAL -->|Failure| ERR

    HUMAN["ğŸ‘¤ <b>Human Review</b>\n<i>Manual Verification</i>"]
    HUMAN -->|Approved| STORE
    HUMAN -->|Rejected + Fix| R

    STORE[("ğŸ’¾ <b>Qdrant Store</b>\n<i>Vector Embeddings</i>")]

    ERR["ğŸ¥ <b>Self-Healing</b>\n<i>Retry / Fallback</i>"]
    ERR -->|Retry| V
    ERR -->|"Max Retries\nExceeded"| FAIL

    FAIL(("âŒ Pipeline\nFailed"))

    STORE --> DONE(("âœ… Ready for\nQuerying"))

    style START fill:#00b894,stroke:#00b894,color:#fff
    style DONE fill:#00b894,stroke:#00b894,color:#fff
    style FAIL fill:#d63031,stroke:#d63031,color:#fff
    style ERR fill:#fdcb6e,stroke:#f39c12,color:#2d3436
    style CR fill:#e17055,stroke:#d63031,color:#fff
    style HUMAN fill:#0984e3,stroke:#0984e3,color:#fff
    style STORE fill:#6c5ce7,stroke:#6c5ce7,color:#fff
    style V fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style O fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style L fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style R fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style F fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style VAL fill:#2d3436,stroke:#636e72,color:#dfe6e9
```

---

## âš”ï¸ Conflict Resolution Deep Dive

When OCR and Vision disagree (e.g., a table is detected visually but OCR reads it as plain text), the **Conflict Resolution Engine** arbitrates using semantic logic.

```mermaid
flowchart LR
    subgraph INPUTS["ğŸ“¥ Agent Outputs"]
        VIS["ğŸ‘ï¸ Vision:\nTable at [100,200,500,600]"]
        OCR_OUT["ğŸ“ OCR:\nPlain text stream\n(columns read vertically)"]
    end

    subgraph DETECT["ğŸ” Detection"]
        ANOMALY["âš ï¸ Anomaly Detected:\nVision sees TABLE structure\nbut OCR text is linear"]
    end

    subgraph RESOLVE["âš—ï¸ Resolution Strategy"]
        ALIGN["1ï¸âƒ£ Align OCR boxes\nonto Vision grid"]
        REGROUP["2ï¸âƒ£ Regroup tokens\nby visual rows"]
        VALIDATE_R["3ï¸âƒ£ Validate structure\nwith LLM"]
    end

    subgraph OUTPUT["âœ… Result"]
        TABLE["ğŸ“Š Structured Table JSON\nwith correct row-major order"]
    end

    VIS --> ANOMALY
    OCR_OUT --> ANOMALY
    ANOMALY --> ALIGN
    ALIGN --> REGROUP
    REGROUP --> VALIDATE_R
    VALIDATE_R --> TABLE

    style INPUTS fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style DETECT fill:#d63031,stroke:#d63031,color:#fff
    style RESOLVE fill:#0984e3,stroke:#0984e3,color:#fff
    style OUTPUT fill:#00b894,stroke:#00b894,color:#fff
```

---

## ğŸ” RAG Query Flow

When a user asks a question, the Multi-Modal RAG pipeline retrieves answers from text, tables, and images simultaneously.

```mermaid
flowchart TD
    Q["ğŸ’¬ User Question\n<i>'What was Q3 revenue?'</i>"]
    Q --> EMB["ğŸ”¢ Generate Embedding\n<i>SentenceTransformer</i>"]

    EMB --> SEARCH

    subgraph SEARCH["ğŸ” Cross-Modal Search"]
        S_TEXT["ğŸ“ Text\nCollection"]
        S_TABLE["ğŸ“Š Table\nCollection"]
        S_IMAGE["ğŸ–¼ï¸ Image\nCollection"]
    end

    SEARCH --> RANK["ğŸ† Rank & Fuse Results\n<i>Cross-Modal Retriever</i>"]
    RANK --> LLM["ğŸ§  LLM Synthesis\n<i>Groq / OpenAI / Anthropic</i>"]

    LLM --> ELI5{"ğŸ¯ Mode?"}
    ELI5 -->|"ELI5 Mode"| SIMPLE["ğŸ˜Š Simple Explanation\nwith visual highlights"]
    ELI5 -->|"Expert Mode"| EXPERT["ğŸ”¬ Technical Deep Dive\nwith source citations"]

    SIMPLE --> RESPONSE["ğŸ“¤ Response + Source Map"]
    EXPERT --> RESPONSE

    style Q fill:#6c5ce7,stroke:#6c5ce7,color:#fff
    style EMB fill:#0984e3,stroke:#0984e3,color:#fff
    style SEARCH fill:#2d3436,stroke:#636e72,color:#dfe6e9
    style RANK fill:#e17055,stroke:#d63031,color:#fff
    style LLM fill:#fdcb6e,stroke:#f39c12,color:#2d3436
    style ELI5 fill:#00cec9,stroke:#00cec9,color:#fff
    style SIMPLE fill:#00b894,stroke:#00b894,color:#fff
    style EXPERT fill:#00b894,stroke:#00b894,color:#fff
    style RESPONSE fill:#a29bfe,stroke:#6c5ce7,color:#fff
```

---

## ğŸ›¡ï¸ Technology Stack

<div align="center">

| Layer | Technology | Purpose |
|:---|:---|:---|
| **ğŸ¤– Agent Orchestration** | **LangGraph** | Deterministic state-machine pipeline with cyclic recovery |
| **ğŸ‘¥ Crew Orchestration** | **CrewAI** | High-level multi-agent analysis (Analyst, Fact-Checker, Summarizer) |
| **ğŸ§  LLM Intelligence** | **Llama 3.1** / OpenAI / Anthropic / Groq | Contextual reasoning & natural language generation |
| **ğŸ‘ï¸ Computer Vision** | **YOLOv8** (Ultralytics) + OpenCV | Document layout detection & region classification |
| **ğŸ“ OCR Engine** | **Tesseract** + **EasyOCR** | Hybrid dual-engine text extraction for max accuracy |
| **ğŸ”® Vector Database** | **Qdrant** | Persistent vector storage with cross-modal collections |
| **ğŸ”¢ Embeddings** | **SentenceTransformer** | Semantic embeddings for text, tables, and images |
| **âš¡ API Framework** | **FastAPI** + WebSockets | Async REST API with real-time progress updates |
| **ğŸ¨ Frontend** | **React 18** + Vite + CSS | Premium UI with heatmaps, chat, document viewer |
| **ğŸ³ Containerization** | **Docker** + Docker Compose | One-command deployment with isolated services |
| **ğŸš€ CI/CD** | **GitHub Actions** | Automated testing, linting, and deployment pipelines |
| **ğŸ“„ PDF Processing** | pdf2image + pdfplumber + PyPDF2 | Multi-engine PDF text & image extraction |

</div>

---

## ğŸ“ Project Structure

```
AI-Agent_Builder/
â”‚
â”œâ”€â”€ ğŸ”§ backend/                    # Python Backend
â”‚   â”œâ”€â”€ agents/                    # ğŸ¤– Core AI Agent System
â”‚   â”‚   â”œâ”€â”€ workflow.py            #    LangGraph pipeline definition
â”‚   â”‚   â”œâ”€â”€ vision_agent.py        #    ğŸ‘ï¸ YOLOv8 document layout detection
â”‚   â”‚   â”œâ”€â”€ ocr_agent.py           #    ğŸ“ Dual OCR (Tesseract + EasyOCR)
â”‚   â”‚   â”œâ”€â”€ layout_agent.py        #    ğŸ“ Structure reconstruction
â”‚   â”‚   â”œâ”€â”€ text_reasoning_agent.py#    ğŸ§  LLM-powered text analysis
â”‚   â”‚   â”œâ”€â”€ table_reasoning_agent.py#   ğŸ“Š Table-specific reasoning & queries
â”‚   â”‚   â”œâ”€â”€ fusion_agent.py        #    âš—ï¸ Cross-modal fusion
â”‚   â”‚   â”œâ”€â”€ validation_agent.py    #    âœ… Quality assurance & confidence
â”‚   â”‚   â”œâ”€â”€ conflict_resolution.py #    âš¡ OCR vs Vision arbitration
â”‚   â”‚   â”œâ”€â”€ self_healing.py        #    ğŸ¥ Auto-retry & fallback strategies
â”‚   â”‚   â”œâ”€â”€ crew_orchestrator.py   #    ğŸ‘¥ CrewAI multi-agent crews
â”‚   â”‚   â”œâ”€â”€ voice_agent.py         #    ğŸ¤ Voice query transcription
â”‚   â”‚   â”œâ”€â”€ confidence_heatmap.py  #    ğŸŒ¡ï¸ Visual confidence overlays
â”‚   â”‚   â””â”€â”€ state.py               #    ğŸ“‹ DocumentState definition
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                       # âš¡ FastAPI Application
â”‚   â”‚   â”œâ”€â”€ main.py                #    App entry point & middleware
â”‚   â”‚   â”œâ”€â”€ routes/                #    Endpoint definitions
â”‚   â”‚   â””â”€â”€ websocket.py           #    Real-time progress updates
â”‚   â”‚
â”‚   â”œâ”€â”€ cv/                        # ğŸ‘ï¸ Computer Vision Module
â”‚   â”‚   â””â”€â”€ preprocessor.py        #    Image preprocessing pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ ocr/                       # ğŸ“ OCR Module
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                       # ğŸ” RAG Pipeline
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py        #    Query orchestration
â”‚   â”‚   â”œâ”€â”€ cross_modal_retriever.py#   Multi-collection search
â”‚   â”‚   â”œâ”€â”€ embeddings.py          #    SentenceTransformer embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py        #    Qdrant integration
â”‚   â”‚   â””â”€â”€ retriever.py           #    Base retriever
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                     # ğŸ§ª Backend Tests (pytest)
â”‚   â”œâ”€â”€ config.py                  #    Environment configuration
â”‚   â””â”€â”€ requirements.txt           #    Python dependencies
â”‚
â”œâ”€â”€ ğŸ¨ frontend/                   # React Frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ DocumentUpload.jsx  #    ğŸ“¤ Drag & drop file upload
â”‚       â”‚   â”œâ”€â”€ ChatInterface.jsx   #    ğŸ’¬ ELI5/Expert chat with RAG
â”‚       â”‚   â”œâ”€â”€ DocumentViewer.jsx  #    ğŸ“„ Rendered document with overlays
â”‚       â”‚   â”œâ”€â”€ ConfidenceHeatmap.jsx#   ğŸŒ¡ï¸ Visual confidence maps
â”‚       â”‚   â”œâ”€â”€ ReviewPanel.jsx     #    ğŸ“‹ Human-in-the-loop review
â”‚       â”‚   â”œâ”€â”€ MultiDocumentCompare.jsx # ğŸ” Side-by-side comparison
â”‚       â”‚   â””â”€â”€ Header.jsx          #    ğŸ” Navigation & theme toggle
â”‚       â”œâ”€â”€ App.jsx                 #    Root component
â”‚       â””â”€â”€ index.css               #    Design system & tokens
â”‚
â”œâ”€â”€ ğŸ³ docker/                     # Docker Configuration
â”‚   â”œâ”€â”€ Dockerfile.backend         #    Backend container
â”‚   â””â”€â”€ Dockerfile.frontend        #    Frontend container
â”‚
â”œâ”€â”€ âš™ï¸ .github/workflows/          # CI/CD Pipelines
â”œâ”€â”€ docker-compose.yml             # One-command stack launch
â”œâ”€â”€ Makefile                       # Developer shortcuts
â””â”€â”€ technical_report.md            # Architecture Decision Records
```

---

## âš¡ Quick Start

### Prerequisites

- **Python 3.11+** Â· **Node.js 18+** Â· **Docker** (optional)
- An API key from **Groq**, **OpenAI**, or **Anthropic**

---

### ğŸ³ Option 1: Docker (Recommended)

Get the entire stack running in **2 minutes**.

```bash
# 1. Clone the repository
git clone https://github.com/gopika-repo/AI-Agent_Builder.git
cd AI-Agent_Builder

# 2. Configure environment
cp backend/.env.example backend/.env
# Edit backend/.env and add your API key:
#   GROQ_API_KEY=your_key_here

# 3. Launch the full stack
docker-compose up -d --build
```

> **ğŸŒ Access Points:**
> | Service | URL |
> |:---|:---|
> | Frontend | `http://localhost:3000` |
> | API Docs | `http://localhost:8000/docs` |
> | API Health | `http://localhost:8000/health` |

---

### ğŸ’» Option 2: Local Development

For developers who want to modify agents and iterate quickly.

**Backend:**
```bash
cd backend
python -m venv venv
source venv/bin/activate    # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn api.main:app --reload --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm run dev
```

---

### ğŸ”§ Environment Variables

Create `backend/.env` with:

```env
# Required â€” at least one LLM provider
GROQ_API_KEY=your_groq_api_key

# Optional â€” additional providers
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key

# Optional â€” Qdrant (defaults to in-memory)
QDRANT_HOST=localhost
QDRANT_PORT=6333
```

---

## ğŸ“¡ API Reference

| Method | Endpoint | Description |
|:---|:---|:---|
| `POST` | `/api/documents/upload` | Upload a document (PDF, PNG, JPG) |
| `GET` | `/api/documents/{id}/status` | Check processing status |
| `GET` | `/api/documents/{id}/results` | Get extraction results |
| `POST` | `/api/chat/{id}/query` | Ask a question (ELI5 / Expert) |
| `POST` | `/api/chat/{id}/voice` | Voice query (audio upload) |
| `GET` | `/api/documents/{id}/confidence` | Get confidence heatmap |
| `POST` | `/api/review/{id}/submit` | Submit human review decision |
| `WS` | `/ws/{document_id}` | Real-time processing updates |

> ğŸ“– **Interactive docs:** Visit `http://localhost:8000/docs` for the full Swagger UI

---

## ğŸ§ª Running Tests

```bash
# Backend tests
cd backend
pytest tests/ -v --cov=. --cov-report=term-missing

# Frontend tests
cd frontend
npm test

# Full suite via Makefile
make test
```

---

## ğŸ“Š Benchmarks

The project includes a **quantitative evaluation suite** for measuring pipeline accuracy against ground-truth datasets.

```bash
# Run the evaluation benchmark
python -m benchmarks.evaluate_pipeline

# Custom dataset
python -m benchmarks.evaluate_pipeline --dataset path/to/ground_truth.json
```

| Metric | What It Measures |
|:---|:---|
| **Text Extraction F1** | Word-level precision, recall, and character accuracy |
| **Table Structure Accuracy** | Cell-level F1, row/column correctness |
| **Layout Detection IoU** | Bounding box accuracy for detected regions |
| **Conflict Resolution Rate** | Detection rate, resolution accuracy, confidence delta |

> ğŸ“– See [`benchmarks/README.md`](benchmarks/README.md) for full documentation.

---

## ğŸš¢ Deployment

<details>
<summary><b>ğŸ”µ Deploy Backend to Render</b></summary>

1. Create a new **Web Service** on [Render](https://render.com)
2. Connect your GitHub repository
3. Set build command: `pip install -r backend/requirements.txt`
4. Set start command: `uvicorn backend.api.main:app --host 0.0.0.0 --port $PORT`
5. Add environment variables (`GROQ_API_KEY`, etc.)

</details>

<details>
<summary><b>ğŸŸ£ Deploy Frontend to Vercel</b></summary>

1. Import project on [Vercel](https://vercel.com)
2. Set root directory to `frontend`
3. Set framework preset to **Vite**
4. Add environment variable: `VITE_API_URL=https://your-backend.onrender.com`

</details>

---

## ğŸ—ºï¸ Roadmap

- [x] ğŸ¤– 6-Agent LangGraph Pipeline (Vision â†’ OCR â†’ Layout â†’ Reasoning â†’ Fusion â†’ Validation)
- [x] âš¡ Conflict-Aware Fusion Engine
- [x] ğŸ¥ Self-Healing Pipeline with retry & fallback
- [x] ğŸ” Multi-Modal RAG (Text + Table + Image retrieval)
- [x] ğŸŒ¡ï¸ Confidence Heatmaps & Visual Grounding
- [x] ğŸ‘¤ Human-in-the-Loop Review Panel
- [x] ğŸ¤ Voice Query Interface
- [x] ğŸ‘¥ CrewAI Multi-Agent Analysis Crews
- [x] ğŸ“Š Table Reasoning Agent (SQL-like queries on tables)
- [x] ğŸŒ— Light / Dark Mode
- [x] ğŸ³ Docker + Docker Compose with Health Checks
- [x] ğŸš€ CI/CD with GitHub Actions
- [x] ğŸ“Š Quantitative Evaluation Benchmark Suite
- [x] ğŸ“± Responsive UI Design

---

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

```bash
# Setup pre-commit hooks
pip install pre-commit
pre-commit install

# Code formatting
make format

# Linting
make lint
```

---

## ğŸ“„ License

Distributed under the **MIT License**. See [LICENSE](LICENSE) for details.

---

<div align="center">

<br>

**Built with â¤ï¸ using LangGraph, CrewAI, YOLOv8, and Qdrant**

_If this project helped you, consider giving it a â­_

<br>

</div>
